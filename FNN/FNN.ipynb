{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1> Using FNN - Feed Forward Neural Network (LR/GBR)</center></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing,svm\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import seaborn as sns\n",
    "# example of making predictions for a regression problem\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from numpy import array\n",
    "from random import shuffle\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from keras import losses\n",
    "from datetime import datetime, timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.layers import Dense, Dropout\n",
    "from sklearn.ensemble import GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. FNN Architecture:\n",
    "With this content, I using simple FNN with 2 hidden layer. \n",
    "- Layer 1: 1024 node\n",
    "- Layer 2: 512 node\n",
    "- Output: 1 node\n",
    "<img src=\"./img/FNN.png\" alt=\"drawing\" style=\"width:400px;\"/>\n",
    "<center>FNN Architecture</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def def_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(1024, input_dim=10, activation='relu'))\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mse', # Mean squared error\n",
    "                    optimizer='adam', # Optimization algorithm\n",
    "                    metrics=['mse'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"train.csv\", sep=',')\n",
    "test = pd.read_csv(\"test_id.csv\",sep=',')\n",
    "\n",
    "dropHeader = ['Before2Month','Before1Month','Before3Week','Before2Week','Before1Week']\n",
    "dropAll = ['UPDATE_TIME','HOUR_ID','ZONE_CODE','SERVER_NAME','DayOfWeek','DayOfMonth','DayOfYear','And2MBefore','And1MBefore','And3WBefore','And2WBefore','And1WBefore']\n",
    "dropAllLookup = ['UPDATE_TIME','HOUR_ID','ZONE_CODE','SERVER_NAME','DayOfWeek','DayOfMonth','DayOfYear','And2MBefore','And1MBefore','And3WBefore','And2WBefore','And1WBefore','Traffic1WBefore','Traffic2WBefore','Traffic3WBefore','Traffic1MBefore','Traffic2MBefore']\n",
    "feature_output = ['MAX_USER','BANDWIDTH_TOTAL']\n",
    "feature_input = ['HOUR_ID','ZONE_CODE','DayOfWeek','DayOfMonth','DayOfYear','Traffic1WBefore','Traffic2WBefore','Traffic3WBefore','Traffic1MBefore','Traffic2MBefore']\n",
    "all_server_name = test.SERVER_NAME.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_server_name = all_server_name[0:3] #Only for test with small data\n",
    "all_server_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Preprocessing data and make feature:\n",
    "Input of FNN have 10 feature as below:\n",
    "- HOUR_ID:\n",
    "- ZONE_CODE: with Label Encoder\n",
    "- DayOfWeek: Day of week\n",
    "- DayOfMonth: Day of month\n",
    "- DayOfYear: Day of year\n",
    "- Before2Month: Data collected from the last 2 months\n",
    "- Before1Month: Data collected from the last previous months\n",
    "- Before3Week: Data collected from the last 3 weeks\n",
    "- Before2Week: Data collected from the last 2 weeks\n",
    "- Before1Week: Data collected from the last week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for y1 in feature_output:\n",
    "    print(y1)\n",
    "    train_data = train[['UPDATE_TIME','HOUR_ID','ZONE_CODE','SERVER_NAME',y1]]\n",
    "    test_data = test[['UPDATE_TIME','HOUR_ID','ZONE_CODE','SERVER_NAME']]\n",
    "    test_data[y1] = 0\n",
    "    \n",
    "    #concat train & test to process one time\n",
    "    all_data = pd.concat([train_data,test_data])\n",
    "    \n",
    "    #Day of Week/Month/Year\n",
    "    all_data['UPDATE_TIME'] = pd.to_datetime(all_data['UPDATE_TIME'])\n",
    "    all_data['DayOfWeek'] = all_data['UPDATE_TIME'].dt.dayofweek\n",
    "    all_data['DayOfMonth'] = all_data['UPDATE_TIME'].dt.day\n",
    "    all_data['DayOfYear'] = all_data['UPDATE_TIME'].dt.dayofyear\n",
    "    \n",
    "    #Shift time\n",
    "    all_data['Before2Month']=all_data.UPDATE_TIME.apply(lambda x: x + relativedelta(months=-2))\n",
    "    all_data['Before1Month']=all_data.UPDATE_TIME.apply(lambda x: x + relativedelta(months=-1))\n",
    "    all_data['Before3Week']=all_data.UPDATE_TIME.apply(lambda x: x + timedelta(days=-7*3))\n",
    "    all_data['Before2Week']=all_data.UPDATE_TIME.apply(lambda x: x + timedelta(days=-7*2))\n",
    "    all_data['Before1Week']=all_data.UPDATE_TIME.apply(lambda x: x + timedelta(days=-7*1))\n",
    "    \n",
    "    #Make data point\n",
    "    all_data['AndFirst'] = all_data['UPDATE_TIME'].map(str) + all_data['HOUR_ID'].map(str) +  all_data['SERVER_NAME'].map(str)\n",
    "    all_data['And2MBefore'] = all_data['Before2Month'].map(str) + all_data['HOUR_ID'].map(str) +  all_data['SERVER_NAME'].map(str)\n",
    "    all_data['And1MBefore'] = all_data['Before1Month'].map(str) + all_data['HOUR_ID'].map(str) +  all_data['SERVER_NAME'].map(str)\n",
    "    all_data['And3WBefore'] = all_data['Before3Week'].map(str) + all_data['HOUR_ID'].map(str) +  all_data['SERVER_NAME'].map(str)\n",
    "    all_data['And2WBefore'] = all_data['Before2Week'].map(str) + all_data['HOUR_ID'].map(str) +  all_data['SERVER_NAME'].map(str)\n",
    "    all_data['And1WBefore'] = all_data['Before1Week'].map(str) + all_data['HOUR_ID'].map(str) +  all_data['SERVER_NAME'].map(str)\n",
    "    all_data = all_data.drop(dropHeader,axis=1)\n",
    "    dataX = all_data.drop(dropAll, axis=1)\n",
    "    \n",
    "    #Lookup data\n",
    "    for i in range(1,4):\n",
    "        strTraffic = 'Traffic' + str(i) + 'WBefore'\n",
    "        strBeffore = 'And'+ str(i) + 'WBefore'\n",
    "        trainReplace = dataX.rename(columns={y1: strTraffic, 'AndFirst': strBeffore})\n",
    "        all_data = all_data.merge(trainReplace, on=strBeffore, how='left')\n",
    "    for i in range(1,3):\n",
    "        strTraffic = 'Traffic' + str(i) + 'MBefore'\n",
    "        strBeffore = 'And'+ str(i) + 'MBefore'\n",
    "        trainReplace = dataX.rename(columns={y1: strTraffic, 'AndFirst': strBeffore})\n",
    "        all_data = all_data.merge(trainReplace, on=strBeffore, how='left')\n",
    "    \n",
    "    all_data = all_data.dropna(axis=0)\n",
    "    #Label Encoder\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    le.fit(all_data['ZONE_CODE'])\n",
    "    all_data['ZONE_CODE'] = le.transform(all_data['ZONE_CODE']) \n",
    "    all_data.to_csv(\"FNN_DataOK_\" + y1 + \".csv\", index=False)\n",
    "    print('Export: ', \"FNN_DataOK_\" + y1 + \".csv\", all_data.shape)\n",
    "    del all_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Predict\n",
    "- Because we must predict 1 month for every server. This is long time.\n",
    "- We will predict 1 week in future. When we have result, we will concat with input data to make input feature for predict next week.\n",
    "<img src=\"./img/time_array.PNG\" alt=\"drawing\" style=\"width:100%;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_x = test.copy()\n",
    "test_data_x.UPDATE_TIME = pd.to_datetime(test_data_x.UPDATE_TIME)\n",
    "test_data_x[\"AndFirst\"] = test_data_x.UPDATE_TIME.map(str) + test_data_x.HOUR_ID.map(str) + test_data_x.SERVER_NAME.map(str)\n",
    "\n",
    "for y1 in feature_output:\n",
    "    test_data = test[['UPDATE_TIME','HOUR_ID','ZONE_CODE','SERVER_NAME']]\n",
    "    test_data[y1] = 0\n",
    "    all_data = pd.read_csv(\"FNN_DataOK_\" + y1 + \".csv\")\n",
    "    result = all_data.head(1)\n",
    "    print('--------------- Run: ', y1,all_data.shape, '---------------------')\n",
    "    for item in all_server_name:\n",
    "        data_one = all_data[all_data.SERVER_NAME == item]\n",
    "        print(item,data_one.shape)\n",
    "        data_one['UPDATE_TIME'] = pd.to_datetime(data_one['UPDATE_TIME'])\n",
    "        train_model_one = data_one[data_one.UPDATE_TIME < '2019-03-10']\n",
    "        train_model_one_x = train_model_one[feature_input]\n",
    "        train_model_one_y = train_model_one[[y1]]\n",
    "        train_model_one_y = np.asarray(train_model_one_y).reshape(-1,1)\n",
    "        \n",
    "        #Normalization data\n",
    "        scalarX, scalarY = MinMaxScaler(), MinMaxScaler()\n",
    "        scalarX.fit(train_model_one_x)\n",
    "        scalarY.fit(train_model_one_y)\n",
    "        train_model_one_xs = scalarX.transform(train_model_one_x)\n",
    "        train_model_one_ys = scalarY.transform(train_model_one_y)\n",
    "        \n",
    "        #Split train & validation\n",
    "        train_one_xs_new, validate_one_xs, train_one_ys_new, validate_one_ys = train_test_split(train_model_one_xs, train_model_one_ys, test_size=0.1, random_state=0)\n",
    "        accuracy = 0\n",
    "        \n",
    "#         #LINEAR REGRESSION MODEL\n",
    "#         model=LinearRegression()\n",
    "#         model.fit(train_one_xs_new,train_one_ys_new)\n",
    "#         accuracy = model.score(validate_one_xs,validate_one_ys)\n",
    "\n",
    "#         #GradientBoostingRegressor\n",
    "#         model = GradientBoostingRegressor(n_estimators=100, max_depth=3, loss='ls')\n",
    "#         model.fit(train_model_one_xs, train_model_one_ys)\n",
    "        \n",
    "        #FNN\n",
    "        model = def_model()\n",
    "        model.fit(train_one_xs_new, \n",
    "                  train_one_ys_new, \n",
    "                  batch_size=256, \n",
    "                  epochs=5, \n",
    "                  validation_data=(validate_one_xs, validate_one_ys))\n",
    "        \n",
    "        date_time_test = '2019-03-10'\n",
    "        start_date = datetime.strptime(date_time_test, '%Y-%m-%d')\n",
    "        max_date = datetime.strptime('2019-04-10', '%Y-%m-%d')\n",
    "        \n",
    "        end_date = start_date\n",
    "        while(end_date < max_date):\n",
    "            #Predict one week\n",
    "            end_date = start_date + timedelta(days=+7)\n",
    "            if (end_date > max_date): \n",
    "                end_date = max_date\n",
    "            print(item,accuracy,start_date,\"-\",end_date)\n",
    "            dataX_one_data = data_one.drop(dropAllLookup, axis=1)\n",
    "            dataX_one_data.head()\n",
    "            dataX_one_data_rmv = dataX_one_data.drop_duplicates()\n",
    "            test_one = data_one[(data_one.UPDATE_TIME < end_date) & (data_one.UPDATE_TIME >= start_date)]\n",
    "            if (test_one.shape[0] > 0):\n",
    "                test_one = test_one.drop(['Traffic1WBefore','Traffic2WBefore','Traffic3WBefore','Traffic1MBefore','Traffic2MBefore'], axis=1)\n",
    "                for i in range(1,4):\n",
    "                    strTraffic = 'Traffic' + str(i) + 'WBefore'\n",
    "                    strBeffore = 'And'+ str(i) + 'WBefore'\n",
    "                    trainReplace = dataX_one_data_rmv.rename(columns={y1: strTraffic, 'AndFirst': strBeffore})\n",
    "                    test_one = test_one.merge(trainReplace, on=strBeffore, how='left')\n",
    "                for i in range(1,3):\n",
    "                    strTraffic = 'Traffic' + str(i) + 'MBefore'\n",
    "                    strBeffore = 'And'+ str(i) + 'MBefore'\n",
    "                    trainReplace = dataX_one_data_rmv.rename(columns={y1: strTraffic, 'AndFirst': strBeffore})\n",
    "                    test_one = test_one.merge(trainReplace, on=strBeffore, how='left')\n",
    "                test_one_x = test_one[feature_input]\n",
    "\n",
    "                test_one_x[test_one_x == \"\"] = np.NaN\n",
    "                test_one_x = test_one_x.fillna(method='ffill')\n",
    "                test_one_x = test_one_x.fillna(0)\n",
    "                test_one_x = scalarX.transform(test_one_x)\n",
    "\n",
    "                test_one_y = model.predict(test_one_x)\n",
    "\n",
    "                test_one_y_inverse = scalarY.inverse_transform(test_one_y.reshape(-1,1))\n",
    "                data_one.loc[(data_one.UPDATE_TIME < end_date) & (data_one.UPDATE_TIME >= start_date), y1] = test_one_y_inverse\n",
    "            else:\n",
    "                print(\"No data - \", item)\n",
    "            start_date = end_date\n",
    "        \n",
    "        \n",
    "        # concat data\n",
    "        result = pd.concat([result, data_one[data_one.UPDATE_TIME >= date_time_test]])\n",
    "        \n",
    "        # clear data    \n",
    "        del data_one \n",
    "        del train_model_one \n",
    "        del train_model_one_x \n",
    "        del train_model_one_y\n",
    "        del train_model_one_xs\n",
    "        del train_model_one_ys\n",
    "        del model\n",
    "        \n",
    "    result.to_csv('RESULT_' + y1 + '.csv')\n",
    "    lookup_data = result[['AndFirst',y1]]\n",
    "    test_data_x = test_data_x.merge(lookup_data,on='AndFirst',how='left')\n",
    "    del result\n",
    "    del all_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Output & Trick\n",
    "Metric of this contest is MAPE:\n",
    "<img src=\"./img/MAPE.PNG\" alt=\"drawing\" style=\"width:300px;\"/>\n",
    "\n",
    "So, when truth label At is so small, we maybe get MAPE that is so high. To avoid this case, when value of predict so small we set equal 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_x[test_data_x == \"\"] = np.NaN\n",
    "test_data_x = test_data_x.fillna(method=\"ffill\")\n",
    "test_data_x.to_csv(\"Backup_Submit.csv\", index=False)\n",
    "test_data_x_copy = test_data_x[['id', 'BANDWIDTH_TOTAL', 'MAX_USER']].copy()\n",
    "test_data_x_copy.MAX_USER[test_data_x_copy.MAX_USER < 1] = 0\n",
    "test_data_x_copy.BANDWIDTH_TOTAL[test_data_x_copy.BANDWIDTH_TOTAL < 260] = 0\n",
    "test_data_x_copy['label'] = round(test_data_x_copy.BANDWIDTH_TOTAL,2).map(str) + \" \" + round(test_data_x_copy.MAX_USER,2).map(str)\n",
    "test_data_x_copy[['id', 'label']].to_csv(\"FNN_Submit.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
